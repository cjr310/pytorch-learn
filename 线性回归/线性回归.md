  ## 损失函数
在模型训练中，我们需要衡量价格预测值与真实值之间的误差。通常我们会选取一个非负数作为误差，且数值越小表示误差越小。一个常用的选择是平方函数。 它在评估索引为 i 的样本误差的表达式为
![](https://github.com/cjr310/pytorch-learn/raw/master/image/1.png )
 ## 优化函数 - 随机梯度下降
 小批量随机梯度下降（mini-batch stochastic gradient descent）：先选取一组模型参数的初始值，如随机选取；接下来对参数进行多次迭代，使每次迭代都可能降低损失函数的值。在每次迭代中，先随机均匀采样一个由固定数目训练数据样本所组成的小批量（mini-batch）B，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后用此结果与预先设定的一个正数的乘积作为模型参数在本次迭代的减小量。
![](https://github.com/cjr310/pytorch-learn/raw/master/image/2.png )

学习率: η代表在每次优化中，能够学习的步长的大小

批量大小: B是小批量计算中的批量大小batch size

总结一下，优化函数的有以下两个步骤：

    (i)初始化模型参数，一般来说使用随机初始化；
    (ii)我们在数据上迭代多次，通过在负梯度方向移动参数来更新每个参数

关键代码

定义模型
```python
def linreg(X, w, b):
    return torch.mm(X, w) + b
```
随机梯度下降
```python
def sgd(params, lr, batch_size): 
    for param in params:
        param.data -= lr * param.grad / batch_size
```

训练
```python
# super parameters init
lr = 0.03
num_epochs = 5

net = linreg
loss = squared_loss

# training
for epoch in range(num_epochs):  # training repeats num_epochs times
    # in each epoch, all the samples in dataset will be used once
    
    # X is the feature and y is the label of a batch sample
    for X, y in data_iter(batch_size, features, labels):
        l = loss(net(X, w, b), y).sum()  
        # calculate the gradient of batch sample loss 
        l.backward()  
        # using small batch random gradient descent to iter model parameters
        sgd([w, b], lr, batch_size)  
        # reset parameter gradient
        w.grad.data.zero_()
        b.grad.data.zero_()
    train_l = loss(net(features, w, b), labels)
    print('epoch %d, loss %f' % (epoch + 1, train_l.mean().item()))

```